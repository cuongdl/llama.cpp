{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia_VKeOX2z8I",
        "outputId": "2fd7ed27-2c10-4a8f-c6c3-e9118d796515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/134)\u001b[K\rremote: Counting objects:   1% (2/134)\u001b[K\rremote: Counting objects:   2% (3/134)\u001b[K\rremote: Counting objects:   3% (5/134)\u001b[K\rremote: Counting objects:   4% (6/134)\u001b[K\rremote: Counting objects:   5% (7/134)\u001b[K\rremote: Counting objects:   6% (9/134)\u001b[K\rremote: Counting objects:   7% (10/134)\u001b[K\rremote: Counting objects:   8% (11/134)\u001b[K\rremote: Counting objects:   9% (13/134)\u001b[K\rremote: Counting objects:  10% (14/134)\u001b[K\rremote: Counting objects:  11% (15/134)\u001b[K\rremote: Counting objects:  12% (17/134)\u001b[K\rremote: Counting objects:  13% (18/134)\u001b[K\rremote: Counting objects:  14% (19/134)\u001b[K\rremote: Counting objects:  15% (21/134)\u001b[K\rremote: Counting objects:  16% (22/134)\u001b[K\rremote: Counting objects:  17% (23/134)\u001b[K\rremote: Counting objects:  18% (25/134)\u001b[K\rremote: Counting objects:  19% (26/134)\u001b[K\rremote: Counting objects:  20% (27/134)\u001b[K\rremote: Counting objects:  21% (29/134)\u001b[K\rremote: Counting objects:  22% (30/134)\u001b[K\rremote: Counting objects:  23% (31/134)\u001b[K\rremote: Counting objects:  24% (33/134)\u001b[K\rremote: Counting objects:  25% (34/134)\u001b[K\rremote: Counting objects:  26% (35/134)\u001b[K\rremote: Counting objects:  27% (37/134)\u001b[K\rremote: Counting objects:  28% (38/134)\u001b[K\rremote: Counting objects:  29% (39/134)\u001b[K\rremote: Counting objects:  30% (41/134)\u001b[K\rremote: Counting objects:  31% (42/134)\u001b[K\rremote: Counting objects:  32% (43/134)\u001b[K\rremote: Counting objects:  33% (45/134)\u001b[K\rremote: Counting objects:  34% (46/134)\u001b[K\rremote: Counting objects:  35% (47/134)\u001b[K\rremote: Counting objects:  36% (49/134)\u001b[K\rremote: Counting objects:  37% (50/134)\u001b[K\rremote: Counting objects:  38% (51/134)\u001b[K\rremote: Counting objects:  39% (53/134)\u001b[K\rremote: Counting objects:  40% (54/134)\u001b[K\rremote: Counting objects:  41% (55/134)\u001b[K\rremote: Counting objects:  42% (57/134)\u001b[K\rremote: Counting objects:  43% (58/134)\u001b[K\rremote: Counting objects:  44% (59/134)\u001b[K\rremote: Counting objects:  45% (61/134)\u001b[K\rremote: Counting objects:  46% (62/134)\u001b[K\rremote: Counting objects:  47% (63/134)\u001b[K\rremote: Counting objects:  48% (65/134)\u001b[K\rremote: Counting objects:  49% (66/134)\u001b[K\rremote: Counting objects:  50% (67/134)\u001b[K\rremote: Counting objects:  51% (69/134)\u001b[K\rremote: Counting objects:  52% (70/134)\u001b[K\rremote: Counting objects:  53% (72/134)\u001b[K\rremote: Counting objects:  54% (73/134)\u001b[K\rremote: Counting objects:  55% (74/134)\u001b[K\rremote: Counting objects:  56% (76/134)\u001b[K\rremote: Counting objects:  57% (77/134)\u001b[K\rremote: Counting objects:  58% (78/134)\u001b[K\rremote: Counting objects:  59% (80/134)\u001b[K\rremote: Counting objects:  60% (81/134)\u001b[K\rremote: Counting objects:  61% (82/134)\u001b[K\rremote: Counting objects:  62% (84/134)\u001b[K\rremote: Counting objects:  63% (85/134)\u001b[K\rremote: Counting objects:  64% (86/134)\u001b[K\rremote: Counting objects:  65% (88/134)\u001b[K\rremote: Counting objects:  66% (89/134)\u001b[K\rremote: Counting objects:  67% (90/134)\u001b[K\rremote: Counting objects:  68% (92/134)\u001b[K\rremote: Counting objects:  69% (93/134)\u001b[K\rremote: Counting objects:  70% (94/134)\u001b[K\rremote: Counting objects:  71% (96/134)\u001b[K\rremote: Counting objects:  72% (97/134)\u001b[K\rremote: Counting objects:  73% (98/134)\u001b[K\rremote: Counting objects:  74% (100/134)\u001b[K\rremote: Counting objects:  75% (101/134)\u001b[K\rremote: Counting objects:  76% (102/134)\u001b[K\rremote: Counting objects:  77% (104/134)\u001b[K\rremote: Counting objects:  78% (105/134)\u001b[K\rremote: Counting objects:  79% (106/134)\u001b[K\rremote: Counting objects:  80% (108/134)\u001b[K\rremote: Counting objects:  81% (109/134)\u001b[K\rremote: Counting objects:  82% (110/134)\u001b[K\rremote: Counting objects:  83% (112/134)\u001b[K\rremote: Counting objects:  84% (113/134)\u001b[K\rremote: Counting objects:  85% (114/134)\u001b[K\rremote: Counting objects:  86% (116/134)\u001b[K\rremote: Counting objects:  87% (117/134)\u001b[K\rremote: Counting objects:  88% (118/134)\u001b[K\rremote: Counting objects:  89% (120/134)\u001b[K\rremote: Counting objects:  90% (121/134)\u001b[K\rremote: Counting objects:  91% (122/134)\u001b[K\rremote: Counting objects:  92% (124/134)\u001b[K\rremote: Counting objects:  93% (125/134)\u001b[K\rremote: Counting objects:  94% (126/134)\u001b[K\rremote: Counting objects:  95% (128/134)\u001b[K\rremote: Counting objects:  96% (129/134)\u001b[K\rremote: Counting objects:  97% (130/134)\u001b[K\rremote: Counting objects:  98% (132/134)\u001b[K\rremote: Counting objects:  99% (133/134)\u001b[K\rremote: Counting objects: 100% (134/134)\u001b[K\rremote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/114)\u001b[K\rremote: Compressing objects:   1% (2/114)\u001b[K\rremote: Compressing objects:   2% (3/114)\u001b[K\rremote: Compressing objects:   3% (4/114)\u001b[K\rremote: Compressing objects:   4% (5/114)\u001b[K\rremote: Compressing objects:   5% (6/114)\u001b[K\rremote: Compressing objects:   6% (7/114)\u001b[K\rremote: Compressing objects:   7% (8/114)\u001b[K\rremote: Compressing objects:   8% (10/114)\u001b[K\rremote: Compressing objects:   9% (11/114)\u001b[K\rremote: Compressing objects:  10% (12/114)\u001b[K\rremote: Compressing objects:  11% (13/114)\u001b[K\rremote: Compressing objects:  12% (14/114)\u001b[K\rremote: Compressing objects:  13% (15/114)\u001b[K\rremote: Compressing objects:  14% (16/114)\u001b[K\rremote: Compressing objects:  15% (18/114)\u001b[K\rremote: Compressing objects:  16% (19/114)\u001b[K\rremote: Compressing objects:  17% (20/114)\u001b[K\rremote: Compressing objects:  18% (21/114)\u001b[K\rremote: Compressing objects:  19% (22/114)\u001b[K\rremote: Compressing objects:  20% (23/114)\u001b[K\rremote: Compressing objects:  21% (24/114)\u001b[K\rremote: Compressing objects:  22% (26/114)\u001b[K\rremote: Compressing objects:  23% (27/114)\u001b[K\rremote: Compressing objects:  24% (28/114)\u001b[K\rremote: Compressing objects:  25% (29/114)\u001b[K\rremote: Compressing objects:  26% (30/114)\u001b[K\rremote: Compressing objects:  27% (31/114)\u001b[K\rremote: Compressing objects:  28% (32/114)\u001b[K\rremote: Compressing objects:  29% (34/114)\u001b[K\rremote: Compressing objects:  30% (35/114)\u001b[K\rremote: Compressing objects:  31% (36/114)\u001b[K\rremote: Compressing objects:  32% (37/114)\u001b[K\rremote: Compressing objects:  33% (38/114)\u001b[K\rremote: Compressing objects:  34% (39/114)\u001b[K\rremote: Compressing objects:  35% (40/114)\u001b[K\rremote: Compressing objects:  36% (42/114)\u001b[K\rremote: Compressing objects:  37% (43/114)\u001b[K\rremote: Compressing objects:  38% (44/114)\u001b[K\rremote: Compressing objects:  39% (45/114)\u001b[K\rremote: Compressing objects:  40% (46/114)\u001b[K\rremote: Compressing objects:  41% (47/114)\u001b[K\rremote: Compressing objects:  42% (48/114)\u001b[K\rremote: Compressing objects:  43% (50/114)\u001b[K\rremote: Compressing objects:  44% (51/114)\u001b[K\rremote: Compressing objects:  45% (52/114)\u001b[K\rremote: Compressing objects:  46% (53/114)\u001b[K\rremote: Compressing objects:  47% (54/114)\u001b[K\rremote: Compressing objects:  48% (55/114)\u001b[K\rremote: Compressing objects:  49% (56/114)\u001b[K\rremote: Compressing objects:  50% (57/114)\u001b[K\rremote: Compressing objects:  51% (59/114)\u001b[K\rremote: Compressing objects:  52% (60/114)\u001b[K\rremote: Compressing objects:  53% (61/114)\u001b[K\rremote: Compressing objects:  54% (62/114)\u001b[K\rremote: Compressing objects:  55% (63/114)\u001b[K\rremote: Compressing objects:  56% (64/114)\u001b[K\rremote: Compressing objects:  57% (65/114)\u001b[K\rremote: Compressing objects:  58% (67/114)\u001b[K\rremote: Compressing objects:  59% (68/114)\u001b[K\rremote: Compressing objects:  60% (69/114)\u001b[K\rremote: Compressing objects:  61% (70/114)\u001b[K\rremote: Compressing objects:  62% (71/114)\u001b[K\rremote: Compressing objects:  63% (72/114)\u001b[K\rremote: Compressing objects:  64% (73/114)\u001b[K\rremote: Compressing objects:  65% (75/114)\u001b[K\rremote: Compressing objects:  66% (76/114)\u001b[K\rremote: Compressing objects:  67% (77/114)\u001b[K\rremote: Compressing objects:  68% (78/114)\u001b[K\rremote: Compressing objects:  69% (79/114)\u001b[K\rremote: Compressing objects:  70% (80/114)\u001b[K\rremote: Compressing objects:  71% (81/114)\u001b[K\rremote: Compressing objects:  72% (83/114)\u001b[K\rremote: Compressing objects:  73% (84/114)\u001b[K\rremote: Compressing objects:  74% (85/114)\u001b[K\rremote: Compressing objects:  75% (86/114)\u001b[K\rremote: Compressing objects:  76% (87/114)\u001b[K\rremote: Compressing objects:  77% (88/114)\u001b[K\rremote: Compressing objects:  78% (89/114)\u001b[K\rremote: Compressing objects:  79% (91/114)\u001b[K\rremote: Compressing objects:  80% (92/114)\u001b[K\rremote: Compressing objects:  81% (93/114)\u001b[K\rremote: Compressing objects:  82% (94/114)\u001b[K\rremote: Compressing objects:  83% (95/114)\u001b[K\rremote: Compressing objects:  84% (96/114)\u001b[K\rremote: Compressing objects:  85% (97/114)\u001b[K\rremote: Compressing objects:  86% (99/114)\u001b[K\rremote: Compressing objects:  87% (100/114)\u001b[K\rremote: Compressing objects:  88% (101/114)\u001b[K\rremote: Compressing objects:  89% (102/114)\u001b[K\rremote: Compressing objects:  90% (103/114)\u001b[K\rremote: Compressing objects:  91% (104/114)\u001b[K\rremote: Compressing objects:  92% (105/114)\u001b[K\rremote: Compressing objects:  93% (107/114)\u001b[K\rremote: Compressing objects:  94% (108/114)\u001b[K\rremote: Compressing objects:  95% (109/114)\u001b[K\rremote: Compressing objects:  96% (110/114)\u001b[K\rremote: Compressing objects:  97% (111/114)\u001b[K\rremote: Compressing objects:  98% (112/114)\u001b[K\rremote: Compressing objects:  99% (113/114)\u001b[K\rremote: Compressing objects: 100% (114/114)\u001b[K\rremote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "Receiving objects:   0% (1/134)\rReceiving objects:   1% (2/134)\rReceiving objects:   2% (3/134)\rReceiving objects:   3% (5/134)\rReceiving objects:   4% (6/134)\rReceiving objects:   5% (7/134)\rReceiving objects:   6% (9/134)\rReceiving objects:   7% (10/134)\rReceiving objects:   8% (11/134)\rReceiving objects:   9% (13/134)\rReceiving objects:  10% (14/134)\rReceiving objects:  11% (15/134)\rReceiving objects:  12% (17/134)\rReceiving objects:  13% (18/134)\rReceiving objects:  14% (19/134)\rReceiving objects:  15% (21/134)\rReceiving objects:  16% (22/134)\rReceiving objects:  17% (23/134)\rReceiving objects:  18% (25/134)\rReceiving objects:  19% (26/134)\rReceiving objects:  20% (27/134)\rReceiving objects:  21% (29/134)\rReceiving objects:  22% (30/134)\rReceiving objects:  23% (31/134)\rReceiving objects:  24% (33/134)\rReceiving objects:  25% (34/134)\rReceiving objects:  26% (35/134)\rReceiving objects:  27% (37/134)\rReceiving objects:  28% (38/134)\rReceiving objects:  29% (39/134)\rReceiving objects:  30% (41/134)\rReceiving objects:  31% (42/134)\rReceiving objects:  32% (43/134)\rReceiving objects:  33% (45/134)\rReceiving objects:  34% (46/134)\rReceiving objects:  35% (47/134)\rReceiving objects:  36% (49/134)\rReceiving objects:  37% (50/134)\rReceiving objects:  38% (51/134)\rReceiving objects:  39% (53/134)\rReceiving objects:  40% (54/134)\rReceiving objects:  41% (55/134)\rReceiving objects:  42% (57/134)\rReceiving objects:  43% (58/134)\rReceiving objects:  44% (59/134)\rReceiving objects:  45% (61/134)\rReceiving objects:  46% (62/134)\rReceiving objects:  47% (63/134)\rReceiving objects:  48% (65/134)\rReceiving objects:  49% (66/134)\rReceiving objects:  50% (67/134)\rReceiving objects:  51% (69/134)\rReceiving objects:  52% (70/134)\rReceiving objects:  53% (72/134)\rReceiving objects:  54% (73/134)\rReceiving objects:  55% (74/134)\rReceiving objects:  56% (76/134)\rReceiving objects:  57% (77/134)\rReceiving objects:  58% (78/134)\rReceiving objects:  59% (80/134)\rReceiving objects:  60% (81/134)\rReceiving objects:  61% (82/134)\rReceiving objects:  62% (84/134)\rReceiving objects:  63% (85/134)\rReceiving objects:  64% (86/134)\rReceiving objects:  65% (88/134)\rReceiving objects:  66% (89/134)\rReceiving objects:  67% (90/134)\rReceiving objects:  68% (92/134)\rReceiving objects:  69% (93/134)\rReceiving objects:  70% (94/134)\rReceiving objects:  71% (96/134)\rReceiving objects:  72% (97/134)\rReceiving objects:  73% (98/134)\rReceiving objects:  74% (100/134)\rReceiving objects:  75% (101/134)\rReceiving objects:  76% (102/134)\rReceiving objects:  77% (104/134)\rReceiving objects:  78% (105/134)\rReceiving objects:  79% (106/134)\rReceiving objects:  80% (108/134)\rReceiving objects:  81% (109/134)\rReceiving objects:  82% (110/134)\rReceiving objects:  83% (112/134)\rReceiving objects:  84% (113/134)\rReceiving objects:  85% (114/134)\rReceiving objects:  86% (116/134)\rReceiving objects:  87% (117/134)\rReceiving objects:  88% (118/134)\rReceiving objects:  89% (120/134)\rReceiving objects:  90% (121/134)\rReceiving objects:  91% (122/134)\rremote: Total 134 (delta 14), reused 134 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects:  92% (124/134)\rReceiving objects:  93% (125/134)\rReceiving objects:  94% (126/134)\rReceiving objects:  95% (128/134)\rReceiving objects:  96% (129/134)\rReceiving objects:  97% (130/134)\rReceiving objects:  98% (132/134)\rReceiving objects:  99% (133/134)\rReceiving objects: 100% (134/134)\rReceiving objects: 100% (134/134), 1.13 MiB | 20.22 MiB/s, done.\n",
            "Resolving deltas:   0% (0/14)\rResolving deltas:  21% (3/14)\rResolving deltas:  64% (9/14)\rResolving deltas:  71% (10/14)\rResolving deltas:  78% (11/14)\rResolving deltas:  85% (12/14)\rResolving deltas:  92% (13/14)\rResolving deltas: 100% (14/14)\rResolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cuongdl/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tohosScsHyCb",
        "outputId": "313a745c-e7f5-48bc-d591-50f53f2f8c86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 11 root root 4096 Jun 21 04:03 llama.cpp\n",
            "drwxr-xr-x  1 root root 4096 Jun 16 18:16 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezDbnE9iH2rL",
        "outputId": "dd990def-b683-4813-80c7-39f94f211cbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2048\n",
            "-rw-r--r--  1 root root    333 Jun 21 04:03 BUILD_NOTICE.rm\n",
            "-rw-r--r--  1 root root   1808 Jun 21 04:03 build.zig\n",
            "-rw-r--r--  1 root root  15641 Jun 21 04:03 CMakeLists.txt\n",
            "-rw-r--r--  1 root root   4187 Jun 21 04:03 convert-lora-to-ggml.py\n",
            "-rw-r--r--  1 root root    527 Jun 21 04:03 convert-pth-to-ggml.py\n",
            "-rw-r--r--  1 root root  47664 Jun 21 04:03 convert.py\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 docs\n",
            "drwxr-xr-x 14 root root   4096 Jun 21 04:03 examples\n",
            "-rw-r--r--  1 root root   1497 Jun 21 04:03 flake.lock\n",
            "-rw-r--r--  1 root root   2051 Jun 21 04:03 flake.nix\n",
            "-rw-r--r--  1 root root 587296 Jun 21 04:03 ggml.c\n",
            "-rw-r--r--  1 root root  73119 Jun 21 04:03 ggml-cuda.cu\n",
            "-rw-r--r--  1 root root   1403 Jun 21 04:03 ggml-cuda.h\n",
            "-rw-r--r--  1 root root  44726 Jun 21 04:03 ggml.h\n",
            "-rw-r--r--  1 root root   2342 Jun 21 04:03 ggml-metal.h\n",
            "-rw-r--r--  1 root root  36128 Jun 21 04:03 ggml-metal.m\n",
            "-rw-r--r--  1 root root  36048 Jun 21 04:03 ggml-metal.metal\n",
            "-rw-r--r--  1 root root  44258 Jun 21 04:03 ggml-opencl.cpp\n",
            "-rw-r--r--  1 root root    845 Jun 21 04:03 ggml-opencl.h\n",
            "-rw-r--r--  1 root root  86455 Jun 21 04:03 k_quants.c\n",
            "-rw-r--r--  1 root root   5813 Jun 21 04:03 k_quants.h\n",
            "-rw-r--r--  1 root root   1072 Jun 21 04:03 LICENSE\n",
            "-rw-r--r--  1 root root 119358 Jun 21 04:03 llama.cpp\n",
            "-rw-r--r--  1 root root  16516 Jun 21 04:03 llama.h\n",
            "-rw-r--r--  1 root root  13994 Jun 21 04:03 llama-util.h\n",
            "-rw-r--r--  1 root root   8840 Jun 21 04:03 Makefile\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 media\n",
            "-rw-r--r--  1 root root    588 Jun 21 04:03 Package.swift\n",
            "drwxr-xr-x  3 root root   4096 Jun 21 04:03 pocs\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 prompts\n",
            "-rw-r--r--  1 root root  30314 Jun 21 04:03 README.md\n",
            "-rw-r--r--  1 root root     34 Jun 21 04:03 requirements.txt\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 scripts\n",
            "-rw-r--r--  1 root root   3829 Jun 21 04:03 SHA256SUMS\n",
            "-rw-r--r--  1 root root 806984 Jun 21 04:03 shakespeare.txt\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 spm-headers\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "A3uUKnG3IM_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK3aT4v-IAFU",
        "outputId": "572084be-23cd-41a8-a7e7-76e98f4bd517"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW8QZLbjK97d",
        "outputId": "2adbb9e4-174f-4e0b-df23-733e91984d76"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2052\n",
            "-rw-r--r--  1 root root    333 Jun 21 04:03 BUILD_NOTICE.rm\n",
            "-rw-r--r--  1 root root   1808 Jun 21 04:03 build.zig\n",
            "-rw-r--r--  1 root root  15641 Jun 21 04:03 CMakeLists.txt\n",
            "-rw-r--r--  1 root root   4187 Jun 21 04:03 convert-lora-to-ggml.py\n",
            "-rw-r--r--  1 root root    527 Jun 21 04:03 convert-pth-to-ggml.py\n",
            "-rw-r--r--  1 root root  47664 Jun 21 04:03 convert.py\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 docs\n",
            "drwxr-xr-x 14 root root   4096 Jun 21 04:03 examples\n",
            "-rw-r--r--  1 root root   1497 Jun 21 04:03 flake.lock\n",
            "-rw-r--r--  1 root root   2051 Jun 21 04:03 flake.nix\n",
            "-rw-r--r--  1 root root 587296 Jun 21 04:03 ggml.c\n",
            "-rw-r--r--  1 root root  73119 Jun 21 04:03 ggml-cuda.cu\n",
            "-rw-r--r--  1 root root   1403 Jun 21 04:03 ggml-cuda.h\n",
            "-rw-r--r--  1 root root  44726 Jun 21 04:03 ggml.h\n",
            "-rw-r--r--  1 root root   2342 Jun 21 04:03 ggml-metal.h\n",
            "-rw-r--r--  1 root root  36128 Jun 21 04:03 ggml-metal.m\n",
            "-rw-r--r--  1 root root  36048 Jun 21 04:03 ggml-metal.metal\n",
            "-rw-r--r--  1 root root  44258 Jun 21 04:03 ggml-opencl.cpp\n",
            "-rw-r--r--  1 root root    845 Jun 21 04:03 ggml-opencl.h\n",
            "-rw-r--r--  1 root root  86455 Jun 21 04:03 k_quants.c\n",
            "-rw-r--r--  1 root root   5813 Jun 21 04:03 k_quants.h\n",
            "-rw-r--r--  1 root root   1072 Jun 21 04:03 LICENSE\n",
            "-rw-r--r--  1 root root 119358 Jun 21 04:03 llama.cpp\n",
            "-rw-r--r--  1 root root  16516 Jun 21 04:03 llama.h\n",
            "-rw-r--r--  1 root root  13994 Jun 21 04:03 llama-util.h\n",
            "-rw-r--r--  1 root root   8840 Jun 21 04:03 Makefile\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 media\n",
            "-rw-r--r--  1 root root    588 Jun 21 04:03 Package.swift\n",
            "drwxr-xr-x  3 root root   4096 Jun 21 04:03 pocs\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 prompts\n",
            "-rw-r--r--  1 root root  30314 Jun 21 04:03 README.md\n",
            "-rw-r--r--  1 root root     34 Jun 21 04:03 requirements.txt\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 scripts\n",
            "-rw-r--r--  1 root root   3829 Jun 21 04:03 SHA256SUMS\n",
            "-rw-r--r--  1 root root 806984 Jun 21 04:03 shakespeare.txt\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 spm-headers\n",
            "drwxr-xr-x  2 root root   4096 Jun 21 04:03 tests\n",
            "drwxr-xr-x  5 root root   4096 Jun 21 04:07 venv\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.24 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.24.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJrugRKSLuxo",
        "outputId": "be3fa6fd-224a-4ee6-be66-78b9a143c599"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGqsfpsfL0zj",
        "outputId": "cc0e2cd8-5b1c-4b5c-fd69-e6d77e3dea57"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -c examples/common.cpp -o common.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir build; cd build; cmake .. -DBUILD_TESTING=ON -DLLAMA_BUILD_EXAMPLES=ON; cmake --build ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBwQ-24ZL5vy",
        "outputId": "59f1c55f-6dc9-45e2-9e98-d3d1e3e86287"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.25.1\") \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  2%] Built target BUILD_INFO\n",
            "[  5%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/k_quants.c.o\u001b[0m\n",
            "[  8%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[ 13%] Built target llama\n",
            "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 19%] Built target test-quantize-fns\n",
            "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 25%] Built target test-quantize-perf\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 30%] Built target test-sampling\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-tokenizer-0.cpp:19:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n",
            "   19 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-0\n",
            "[ 38%] \u001b[32mBuilding CXX object examples/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 38%] Built target common\n",
            "[ 41%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 44%] Built target main\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 50%] Built target quantize\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 55%] Built target quantize-stats\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 61%] Built target perplexity\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[ 66%] Built target embedding\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
            "[ 72%] Built target save-load-state\n",
            "[ 75%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
            "[ 77%] Built target benchmark\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/examples/baby-llama/baby-llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/examples/baby-llama/baby-llama.cpp:1598:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kopt_params_adam\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            " 1598 |         struct ggml_opt_params \u001b[01;35m\u001b[Kopt_params_adam\u001b[m\u001b[K = ggml_opt_default_params(GGML_OPT_ADAM);\n",
            "      |                                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
            "[ 83%] Built target baby-llama\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/string.h:495\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/include/c++/9/cstring:42\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:7\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:303:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:304:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kchar* strncpy(char*, const char*, size_t)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid init_model(my_llama_model*)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/llama.cpp/examples/train-text-from-scratch/train-text-from-scratch.cpp:305:16\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kchar* __builtin_strncpy(char*, const char*, long unsigned int)\u001b[m\u001b[K’ specified bound 32 equals destination size [\u001b[01;35m\u001b[K-Wstringop-truncation\u001b[m\u001b[K]\n",
            "  106 |   return \u001b[01;35m\u001b[K__builtin___strncpy_chk (__dest, __src, __len, __bos (__dest))\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
            "[ 88%] Built target train-text-from-scratch\n",
            "[ 91%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
            "[ 94%] Built target vdot\n",
            "[ 97%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f models/7B/*\n",
        "!cd models/7B;wget https://huggingface.co/TheBloke/vicuna-7B-1.1-GGML/resolve/main/vicuna-7b-1.1.ggmlv3.q2_K.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCc0BT6rNEmM",
        "outputId": "4ac45550-1c7b-482b-e339-a79c87808167"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-21 05:04:39--  https://huggingface.co/TheBloke/vicuna-7B-1.1-GGML/resolve/main/vicuna-7b-1.1.ggmlv3.q2_K.bin\n",
            "Resolving huggingface.co (huggingface.co)... 108.138.64.67, 108.138.64.89, 108.138.64.87, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.138.64.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/5c/fb/5cfb4588662895e6b141d01acdf6ea1f2b9dcba00b76e3c939702953ea7c1f12/5ab094f0bc6428206476c4b7bf28b4e701f7cf3e5903969e328307fd1263e880?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vicuna-7b-1.1.ggmlv3.q2_K.bin%3B+filename%3D%22vicuna-7b-1.1.ggmlv3.q2_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687583080&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzVjL2ZiLzVjZmI0NTg4NjYyODk1ZTZiMTQxZDAxYWNkZjZlYTFmMmI5ZGNiYTAwYjc2ZTNjOTM5NzAyOTUzZWE3YzFmMTIvNWFiMDk0ZjBiYzY0MjgyMDY0NzZjNGI3YmYyOGI0ZTcwMWY3Y2YzZTU5MDM5NjllMzI4MzA3ZmQxMjYzZTg4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc1ODMwODB9fX1dfQ__&Signature=pqhdfypIXHbBkEyiTDPMQG2VIGh3j3fycxaSzL-lp-FDyNLcUQgWjjIKnJixfDv5h0gRHcUuI-JYUpTm4oTzWCdg-6dgJ6gZvyFnl3kOEgRqev2FraUj2zWBgQMfPMrmGB0z5Q2DF2Hg13R7ZIU5NlolWSYKOxT4owJaOZZqM9-IcY0NlVAUHSk5PljYoK-4dQ63Wyb0LifB1FaborfAoqFHUxZdZUUtOmMw5K4qt9ZXg3AHimZB0zly2AQ5omM16yFlT9WuWLQVgYrOk5vdKq4mxPS1mjfiXZgWD1rc%7Ema9rcbPJbtSppHyhl2YHAA9sGLG-7t8VF3OH7QFkNWHNw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-21 05:04:39--  https://cdn-lfs.huggingface.co/repos/5c/fb/5cfb4588662895e6b141d01acdf6ea1f2b9dcba00b76e3c939702953ea7c1f12/5ab094f0bc6428206476c4b7bf28b4e701f7cf3e5903969e328307fd1263e880?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vicuna-7b-1.1.ggmlv3.q2_K.bin%3B+filename%3D%22vicuna-7b-1.1.ggmlv3.q2_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687583080&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzVjL2ZiLzVjZmI0NTg4NjYyODk1ZTZiMTQxZDAxYWNkZjZlYTFmMmI5ZGNiYTAwYjc2ZTNjOTM5NzAyOTUzZWE3YzFmMTIvNWFiMDk0ZjBiYzY0MjgyMDY0NzZjNGI3YmYyOGI0ZTcwMWY3Y2YzZTU5MDM5NjllMzI4MzA3ZmQxMjYzZTg4MD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODc1ODMwODB9fX1dfQ__&Signature=pqhdfypIXHbBkEyiTDPMQG2VIGh3j3fycxaSzL-lp-FDyNLcUQgWjjIKnJixfDv5h0gRHcUuI-JYUpTm4oTzWCdg-6dgJ6gZvyFnl3kOEgRqev2FraUj2zWBgQMfPMrmGB0z5Q2DF2Hg13R7ZIU5NlolWSYKOxT4owJaOZZqM9-IcY0NlVAUHSk5PljYoK-4dQ63Wyb0LifB1FaborfAoqFHUxZdZUUtOmMw5K4qt9ZXg3AHimZB0zly2AQ5omM16yFlT9WuWLQVgYrOk5vdKq4mxPS1mjfiXZgWD1rc%7Ema9rcbPJbtSppHyhl2YHAA9sGLG-7t8VF3OH7QFkNWHNw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.36, 108.138.64.121, 108.138.64.111, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2802295424 (2.6G) [application/octet-stream]\n",
            "Saving to: ‘vicuna-7b-1.1.ggmlv3.q2_K.bin’\n",
            "\n",
            "vicuna-7b-1.1.ggmlv 100%[===================>]   2.61G  44.2MB/s    in 53s     \n",
            "\n",
            "2023-06-21 05:05:33 (50.5 MB/s) - ‘vicuna-7b-1.1.ggmlv3.q2_K.bin’ saved [2802295424/2802295424]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./models/7B/\n",
        "!./main -t 10 -ngl 32 -m ./models/7B/vicuna-7b-1.1.ggmlv3.q2_K.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"### Instruction: Write a story about llamas\\n### Response:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTz6kyUNRJ-w",
        "outputId": "aac712e7-ebe7-47e7-d47f-a7c7a31fc117"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2736624\n",
            "-rw-r--r-- 1 root root 2802295424 Jun  7 15:52 vicuna-7b-1.1.ggmlv3.q2_K.bin\n",
            "warning: not compiled with GPU offload support, --n-gpu-layers option will be ignored\n",
            "warning: see main README.md for information on enabling GPU BLAS support\n",
            "main: build = 1 (63f8f94)\n",
            "main: seed  = 1687324084\n",
            "llama.cpp: loading model from ./models/7B/vicuna-7b-1.1.ggmlv3.q2_K.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 2048\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
            "llama_model_load_internal: mem required  = 4464.12 MB (+ 1026.00 MB per state)\n",
            "...................................................................................................\n",
            "llama_init_from_file: kv self size  = 1024.00 MB\n",
            "\n",
            "system_info: n_threads = 10 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m ### Instruction: Write a story about llamas\\n### Response:\u001b[0m I wrote a^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m ./models/7B/vicuna-7b-1.1.ggmlv3.q2_K.bin \\\n",
        "        -t 8 \\\n",
        "        -n 128 \\\n",
        "        -p 'The first man on the moon was '"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPfOUlX1XhBu",
        "outputId": "e850d023-8d55-4b29-8e3d-981f69b40f7c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1 (63f8f94)\n",
            "main: seed  = 1687324352\n",
            "llama.cpp: loading model from ./models/7B/vicuna-7b-1.1.ggmlv3.q2_K.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
            "llama_model_load_internal: mem required  = 4464.12 MB (+ 1026.00 MB per state)\n",
            "...................................................................................................\n",
            "llama_init_from_file: kv self size  =  256.00 MB\n",
            "\n",
            "system_info: n_threads = 8 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            " The first man on the moon was 1^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd build;cmake .. -DBUILD_TESTING=ON -DLLAMA_BUILD_EXAMPLES=ON; cmake --build ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaOu9cN6X7BD",
        "outputId": "c14c3ec0-932b-4ad9-9d29-dc7429645ec0"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  2%] Built target BUILD_INFO\n",
            "[  8%] Built target ggml\n",
            "[ 13%] Built target llama\n",
            "[ 19%] Built target test-quantize-fns\n",
            "[ 25%] Built target test-quantize-perf\n",
            "[ 30%] Built target test-sampling\n",
            "[ 36%] Built target test-tokenizer-0\n",
            "[ 38%] Built target common\n",
            "[ 44%] Built target main\n",
            "[ 50%] Built target quantize\n",
            "[ 55%] Built target quantize-stats\n",
            "[ 61%] Built target perplexity\n",
            "[ 66%] Built target embedding\n",
            "[ 72%] Built target save-load-state\n",
            "[ 77%] Built target benchmark\n",
            "[ 83%] Built target baby-llama\n",
            "[ 88%] Built target train-text-from-scratch\n",
            "[ 94%] Built target vdot\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd build; wget https://github.com/brunoklein99/deep-learning-notes/blob/master/shakespeare.txt\n"
      ],
      "metadata": {
        "id": "ATvRzuJxitmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd build; ./bin/train-text-from-scratch \\\n",
        "        --vocab-model ../models/ggml-vocab.bin \\\n",
        "        --ctx 64 --embd 256 --head 8 --layer 16 \\\n",
        "        --checkpoint-in  chk-shakespeare-256x16.bin \\\n",
        "        --checkpoint-out chk-shakespeare-256x16.bin \\\n",
        "        --model-out ggml-shakespeare-256x16-f32.bin \\\n",
        "        --train-data \"../LICENSE\" \\\n",
        "        -t 4 -b 32 -n 32 --seed 1 --adam-iter 16 \\\n",
        "        --print-details-interval 0 --predict 16 --use-flash\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyrzqSk-Yg6Q",
        "outputId": "298726c7-f003-40c7-b75d-85b61be7792c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: seed: 1\n",
            "llama.cpp: loading model from ../models/ggml-vocab.bin\n",
            "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 4096\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 32\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
            "llama_model_load_internal: n_ff       = 11008\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "main: tokenize training data\n",
            "main: number of training tokens: 344\n",
            "print_params: n_vocab: 32000\n",
            "print_params: n_ctx:   64\n",
            "print_params: n_embd:  256\n",
            "print_params: n_mult:  256\n",
            "print_params: n_head:  8\n",
            "print_params: n_ff:    768\n",
            "print_params: n_layer: 16\n",
            "print_params: n_rot:   32\n",
            "main: number of unique tokens: 204\n",
            "main: init model\n",
            "load_checkpoint: Loading model from 'chk-shakespeare-256x16.bin'.\n",
            "print_params: n_vocab: 32000\n",
            "print_params: n_ctx:   64\n",
            "print_params: n_embd:  256\n",
            "print_params: n_mult:  256\n",
            "print_params: n_head:  8\n",
            "print_params: n_ff:    768\n",
            "print_params: n_layer: 16\n",
            "print_params: n_rot:   32\n",
            "load_checkpoint: Training iterations: 1.\n",
            "load_checkpoint: Training samples:    32.\n",
            "load_checkpoint: Training tokens:     1024.\n",
            "main: opt iter 1\n",
            "used_mem model+cache: 1083036416 bytes\n",
            "main: begin training\n",
            "main: opt->params.adam.sched 0.01000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd build; ./bin/quantize ggml-shakespeare-256x16-f32.bin ggml-shakespeare-256x16-f32_q4k.bin 15\n",
        "!ls -l build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl-wx8xeaPIc",
        "outputId": "b5c1ecbd-b4b6-44c1-d89c-616bf70699be"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1 (63f8f94)\n",
            "main: quantizing 'ggml-shakespeare-256x16-f32.bin' to 'ggml-shakespeare-256x16-f32_q4k.bin' as q4_K\n",
            "llama.cpp: loading model from ggml-shakespeare-256x16-f32.bin\n",
            "llama.cpp: saving model to ggml-shakespeare-256x16-f32_q4k.bin\n",
            "[   1/ 147]                tok_embeddings.weight -      256 x 32000, type =    f32, quantizing .. size =    31.25 MB ->     4.39 MB | hist: \n",
            "[   2/ 147]                          norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[   3/ 147]                        output.weight -      256 x 32000, type =    f32, quantizing .. size =    31.25 MB ->     4.39 MB | hist: \n",
            "[   4/ 147]       layers.0.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[   5/ 147]         layers.0.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[   6/ 147]         layers.0.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[   7/ 147]         layers.0.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[   8/ 147]         layers.0.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[   9/ 147]             layers.0.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  10/ 147]      layers.0.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  11/ 147]      layers.0.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[  12/ 147]      layers.0.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  13/ 147]       layers.1.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  14/ 147]         layers.1.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  15/ 147]         layers.1.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  16/ 147]         layers.1.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[  17/ 147]         layers.1.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  18/ 147]             layers.1.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  19/ 147]      layers.1.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  20/ 147]      layers.1.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[  21/ 147]      layers.1.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  22/ 147]       layers.2.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  23/ 147]         layers.2.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  24/ 147]         layers.2.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  25/ 147]         layers.2.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  26/ 147]         layers.2.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  27/ 147]             layers.2.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  28/ 147]      layers.2.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  29/ 147]      layers.2.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  30/ 147]      layers.2.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  31/ 147]       layers.3.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  32/ 147]         layers.3.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  33/ 147]         layers.3.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  34/ 147]         layers.3.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  35/ 147]         layers.3.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  36/ 147]             layers.3.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  37/ 147]      layers.3.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  38/ 147]      layers.3.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  39/ 147]      layers.3.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  40/ 147]       layers.4.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  41/ 147]         layers.4.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  42/ 147]         layers.4.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  43/ 147]         layers.4.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[  44/ 147]         layers.4.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  45/ 147]             layers.4.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  46/ 147]      layers.4.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  47/ 147]      layers.4.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[  48/ 147]      layers.4.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  49/ 147]       layers.5.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  50/ 147]         layers.5.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  51/ 147]         layers.5.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  52/ 147]         layers.5.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  53/ 147]         layers.5.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  54/ 147]             layers.5.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  55/ 147]      layers.5.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  56/ 147]      layers.5.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  57/ 147]      layers.5.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  58/ 147]       layers.6.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  59/ 147]         layers.6.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  60/ 147]         layers.6.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  61/ 147]         layers.6.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  62/ 147]         layers.6.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  63/ 147]             layers.6.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  64/ 147]      layers.6.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  65/ 147]      layers.6.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  66/ 147]      layers.6.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  67/ 147]       layers.7.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  68/ 147]         layers.7.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  69/ 147]         layers.7.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  70/ 147]         layers.7.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[  71/ 147]         layers.7.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  72/ 147]             layers.7.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  73/ 147]      layers.7.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  74/ 147]      layers.7.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[  75/ 147]      layers.7.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  76/ 147]       layers.8.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  77/ 147]         layers.8.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  78/ 147]         layers.8.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  79/ 147]         layers.8.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  80/ 147]         layers.8.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  81/ 147]             layers.8.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  82/ 147]      layers.8.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  83/ 147]      layers.8.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  84/ 147]      layers.8.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  85/ 147]       layers.9.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  86/ 147]         layers.9.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  87/ 147]         layers.9.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  88/ 147]         layers.9.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  89/ 147]         layers.9.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  90/ 147]             layers.9.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  91/ 147]      layers.9.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  92/ 147]      layers.9.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  93/ 147]      layers.9.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[  94/ 147]      layers.10.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[  95/ 147]        layers.10.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  96/ 147]        layers.10.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  97/ 147]        layers.10.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[  98/ 147]        layers.10.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[  99/ 147]            layers.10.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 100/ 147]     layers.10.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 101/ 147]     layers.10.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[ 102/ 147]     layers.10.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 103/ 147]      layers.11.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 104/ 147]        layers.11.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 105/ 147]        layers.11.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 106/ 147]        layers.11.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 107/ 147]        layers.11.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 108/ 147]            layers.11.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 109/ 147]     layers.11.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 110/ 147]     layers.11.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 111/ 147]     layers.11.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 112/ 147]      layers.12.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 113/ 147]        layers.12.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 114/ 147]        layers.12.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 115/ 147]        layers.12.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 116/ 147]        layers.12.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 117/ 147]            layers.12.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 118/ 147]     layers.12.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 119/ 147]     layers.12.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 120/ 147]     layers.12.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 121/ 147]      layers.13.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 122/ 147]        layers.13.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 123/ 147]        layers.13.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 124/ 147]        layers.13.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[ 125/ 147]        layers.13.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 126/ 147]            layers.13.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 127/ 147]     layers.13.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 128/ 147]     layers.13.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[ 129/ 147]     layers.13.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 130/ 147]      layers.14.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 131/ 147]        layers.14.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 132/ 147]        layers.14.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 133/ 147]        layers.14.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[ 134/ 147]        layers.14.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 135/ 147]            layers.14.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 136/ 147]     layers.14.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 137/ 147]     layers.14.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[ 138/ 147]     layers.14.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 139/ 147]      layers.15.attention_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 140/ 147]        layers.15.attention.wq.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 141/ 147]        layers.15.attention.wk.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 142/ 147]        layers.15.attention.wv.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.05 MB | hist: \n",
            "[ 143/ 147]        layers.15.attention.wo.weight -      256 x   256, type =    f32, quantizing .. size =     0.25 MB ->     0.04 MB | hist: \n",
            "[ 144/ 147]            layers.15.ffn_norm.weight -              256, type =    f32, size =    0.001 MB\n",
            "[ 145/ 147]     layers.15.feed_forward.w1.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "[ 146/ 147]     layers.15.feed_forward.w2.weight -      768 x   256, type =    f32, quantizing .. size =     0.75 MB ->     0.15 MB | hist: \n",
            "[ 147/ 147]     layers.15.feed_forward.w3.weight -      256 x   768, type =    f32, quantizing .. size =     0.75 MB ->     0.11 MB | hist: \n",
            "llama_model_quantize_internal: model size  =   114.53 MB\n",
            "llama_model_quantize_internal: quant size  =    16.65 MB\n",
            "\n",
            "main: quantize time =   780.78 ms\n",
            "main:    total time =   780.78 ms\n",
            "total 1075116\n",
            "drwxr-xr-x  2 root root      4096 Jun 21 04:22 bin\n",
            "-rw-r--r--  1 root root 960775756 Jun 21 07:44 chk-shakespeare-256x16.bin\n",
            "-rw-r--r--  1 root root     18290 Jun 21 04:22 CMakeCache.txt\n",
            "drwxr-xr-x 35 root root      4096 Jun 21 05:16 CMakeFiles\n",
            "-rw-r--r--  1 root root      1910 Jun 21 04:22 cmake_install.cmake\n",
            "-rw-r--r--  1 root root      8632 Jun 21 05:16 compile_commands.json\n",
            "-rw-r--r--  1 root root       308 Jun 21 04:22 CTestTestfile.cmake\n",
            "-rw-r--r--  1 root root      2572 Jun 21 04:22 DartConfiguration.tcl\n",
            "drwxr-xr-x 12 root root      4096 Jun 21 05:16 examples\n",
            "-rw-r--r--  1 root root 120537728 Jun 21 07:44 ggml-shakespeare-256x16-f32.bin\n",
            "-rw-r--r--  1 root root  17900160 Jun 21 07:52 ggml-shakespeare-256x16-f32_q4k.bin\n",
            "-rw-r--r--  1 root root    776986 Jun 21 04:22 libllama.a\n",
            "-rw-r--r--  1 root root     29922 Jun 21 05:16 Makefile\n",
            "drwxr-xr-x  4 root root      4096 Jun 21 05:16 pocs\n",
            "-rw-r--r--  1 root root    806967 Jun 21 05:56 shakespeare.txt\n",
            "drwxr-xr-x  3 root root      4096 Jun 21 04:22 Testing\n",
            "drwxr-xr-x  3 root root      4096 Jun 21 05:16 tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m ./build/ggml-shakespeare-256x16-f32_q4k.bin \\\n",
        "        -t 8 \\\n",
        "        -n 128 \\\n",
        "        -p 'The first man on the moon was'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luNFcTls8i_y",
        "outputId": "cfb0b63a-cc09-44b9-f29f-e61589daf16d"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1 (63f8f94)\n",
            "main: seed  = 1687335971\n",
            "llama.cpp: loading model from ./build/ggml-shakespeare-256x16-f32_q4k.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 256\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 8\n",
            "llama_model_load_internal: n_layer    = 16\n",
            "llama_model_load_internal: n_rot      = 32\n",
            "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
            "llama_model_load_internal: n_ff       = 768\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =    0.04 MB\n",
            "llama_model_load_internal: mem required  = 1808.69 MB (+ 1026.00 MB per state)\n",
            "..................................................\n",
            "llama_init_from_file: kv self size  =    8.00 MB\n",
            "\n",
            "system_info: n_threads = 8 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "lasseSelector suddenlylack compte threads lo^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('./build/ggml-shakespeare-256x16-f32_q4k.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BDdtx_mMF3Th",
        "outputId": "78bbfb49-df7b-4ab7-cd5b-1b843267538b"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75f26092-5fae-4a7c-9689-c05cff6887bf\", \"ggml-shakespeare-256x16-f32_q4k.bin\", 17900160)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./main -m ./build/ggml-shakespeare-256x16-f32_q4k.bin --mirostat 2 --mirostat 2 --mirostat-lr 0.98 --mirostat-ent 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IEnfm-uHuBX",
        "outputId": "d8ce6bcd-7e31-473c-92ad-1cab7c6a3e92"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1 (63f8f94)\n",
            "main: seed  = 1687337077\n",
            "llama.cpp: loading model from ./build/ggml-shakespeare-256x16-f32_q4k.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 256\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 8\n",
            "llama_model_load_internal: n_layer    = 16\n",
            "llama_model_load_internal: n_rot      = 32\n",
            "llama_model_load_internal: ftype      = 15 (mostly Q4_K - Medium)\n",
            "llama_model_load_internal: n_ff       = 768\n",
            "llama_model_load_internal: n_parts    = 1\n",
            "llama_model_load_internal: model size = 7B\n",
            "llama_model_load_internal: ggml ctx size =    0.04 MB\n",
            "llama_model_load_internal: mem required  = 1808.69 MB (+ 1026.00 MB per state)\n",
            "..................................................\n",
            "llama_init_from_file: kv self size  =    8.00 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 2, mirostat_lr = 0.980000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0\n",
            "\n",
            "\n",
            "unterunter Art lawyer ESwigunterunterkou lawyerlike godlike lawyeristing lawyerlikePCunterunter Meter lawyerimedia Challike lawyer farmrivial iliHigh ESunter WM lawyer     bev lawyerlike achiev lawyerlikemouthunterunterightarrowunterunter bitlike aboveктор lawyerlike seasonsunterunter한unterunter standinglike lawyerouvelleslike lawyer comparinglike lawyerassetsunterunter exceptionslike lawyerpositionunterunter reachshop lawyer----- lawyerlike determineunterunter edificunterunter])like lawyer embrivial ili Mitg ESunter Constantlike lawyer unusunterunter JinunterunterUnderlike lawyerotti lawyerlikeланд abovelike Meteorunterunterf lawyerlikeめlike lawyerقunterunter includedunterunter aconlike lawyer前like lawyerbashlike lawyer scripts lawyerlike width lawyerlikezymunterunterīlike lawyermultipleunterunter Bootlike lawyer получилrivial ili again ESunterbü lawyerlike今 abovelike communauté lawyerlikekteriviallikenotificationlike lawyer współ lawyerlike wy頭pués」 Erik Hijqâtepin제 usually頭 Треpuéspués Valentâte Erikġ頭 oùedad hommes頭купpuéspués protected頭頭 Ве頭頭 Meyerpuéspuésnor頭頭aal씨頭ิ頭頭 decom頭頭nadsecure頭lies頭blogspuésâte Bapt頭頭primarypuésâte laquelle頭頭ethe頭頭 Army頭 Hij Wirtschaft頭頭données usually頭 railpin頭 opp頭 Erik代頭 oùeta hommes頭 impossible頭頭 ic頭頭umably頭頭 также頭頭 evident頭頭 son頭頭�頭頭SELECT頭頭contrâte頭ime頭頭 immediately頭頭 corresponding頭 Ladyviv頭puésowana頭 oùimetâte頭pository Hij頭 août頭 usually many頭 où березняobbsecureidgepin頭ников Erik頭 осіб頭 oùnu頭 oùaba hommes頭 tells頭 où app où頭oden頭 où executing usually頭headers頭 où daar頭 oùma頭 oùVIS頭 oùestivalmboxâte Television où頭Current頭 Lady Nâte oùжение頭 où Lil Hij頭 niemieck頭 oùherit頭 oùNextsecure où \\]obb oùhipspin頭 evident頭 oùпени頭 où Ice où頭мой hommes頭 back頭 où steel頭 où тер usually頭 Solo où頭ruction usually頭だ où頭 Ook頭 où₆頭 où времени Erikundial dynastŁŁwhereŁŁ Studioundialundial hintundialundial$}ŁŁ §undialundial concernsundialundial muzundialundialoliaundialundialTMundialundialendraundialundialwärundialundial�undialundial printlnundialundialaineŁundial attra autundial classicalundialundial indexingundialundial measurementsundialundialゃundialundialridesundialundial previousundialundial kunnenundialundialнийundialundial á zahlundial Браundialundial Евundialundial inclusionundialundialacionalundialundial Jasonundialundial niundialundialälleundialundial Alterundialundial howeverundialundial Initializeundialundial куundialundialcouŁundialtrace autundial\n",
            "nonnon supnonnon nominsteps周<\\nonnongeschnonnon milesnonnonaxynonnon Gebäudenonnonesinonnon системnonnonPythonnonnonPATHiciannon Windownonnon dtypenonnonimannonnon quelquesnonnonSplitnonnonkwargsnonnonicked flyingnon bonmło Driver Displaynon larnonnonkendenonnon schließlichnonnon\u0004steps周 estánonnon Resłonon ontnonnonFormatternonnon dennnonnonქnonnon composednonnon若nonnon firstnonnonołiciannon Nederlandsenonnon srło leaving desirenon ceremonynon killnonnonhostnon flying solemnonnmylvannonnon Terrnonnon Ashnonnon boundednonnonoslovnonnondziałsteps周 italianononnonałonmnon introductionłonmitätnoninuкерnonnon CatalunyanonnonPtrnm Display countnonnonServernonnon sounonnm males tblnon anon leaving varinonnonchangesiciannon verifiednon flying surroundednonnmтичеnonnmartenmnm миnonnm aktnmnmstringifynmnmstellesteps周рдnonnm*\"nmnm abłonm scenarionmnmsalnmnm whethernmnmʾinunonvonnm Displayikesnonnm«nmnmférencesnmnm Áng leavingilderмож*{OUTzz bu Jungienstilderilderentr blockingilder incomingilderilder:'ilderilderpineilderilderImportilderilderiersilderilder canilderilderhostsilderilder bienilderilderprivateilderilder währendilderilder [`ilderilder Popularilderilder Михаilderilderếilderilder свиilderilder largelyilderilder covilderilderuropeilderilderittestilder*{ pot bu Sen AnsilderilderеруOUTilder remainilderilder acknowilderilder Douilderilderhireilderilderpräsident Jungilderutorsilderilder Roundilderilder omilderilder Wienilderilder районilderilderendenilderilder компаilderilderਰilderilder boatsilderilder clientilderilderтиilderilderViewControllerilderilder черезilderilder�ilder*{ Allen busci descriilderildermanuelOUTilderumsilderilder speilderilder Szilderilder Illilderilder spiritilderilder desarrollilderilderезilderilder necessary伝ilderufeilderilderшіilderilderboilderilder septembreilderilderinfoilderilder Springilderilder ancienilderilderнымиilderilder gatilderilder Gasilderilderète*{ilder Track busci klubilderilder zOUTilder музиilderilder传ilderilder encoreilderilderrèsilderildernsilderilder avaitilderilder acting bu Sen copyingilderilder medievalilderilder Tigilderilderinationilderilderトilder AntiilderilderConsilderilder hyd fortyilder необedeut Ve entrepredeutedeut港edeutedeut wiekuedeutedeut Vicedeutedeut fostedeutedeutableedeutedeut Oktoberedeutedeut citedeutedeut dominedeutedeutumingèveedeutრedeutedeutvirtiedeutedeut IslandedeutedeutUNCTIONedeutedeutuciónedeutedeutEmployeeedeutedeut Spectedeutedeut Chenedeutedeut nuclearedeutedeut BianedeutedeutligtedeutedeutProject Veedeut Minoredeutedeutceredeutedeut Mazedeutedeuthnedeutedeutآedeutedeut nuevoedeutedeut�edeutedeutIteredeutedeutjudedeutedeut handlesedeutedeut letedeutedeutflowedeutedeut rivedeutedeuttałedeutedeutJobèveedeut：edeutedeut seenedeutedeutrayededeutedeut североedeutedeut цьedeutedeutcommandedeutedeut processed Veedeut powiecieedeutedeut rodeedeutedeutこedeutedeut    edeutedeutscredeutedeutifiqueedeutedeutsizeedeutedeutissantchangededeut majaedeutedeutjahredeutedeutcepedeutedeutcuredeutedeutecedeutedeut presentaedeutedeut британèveedeutskihedeutedeut__edeutedeut東edeutedeut investigationedeutedeutclojureedeutedeutnungedeutedeut rag Veedeuthusedeutedeutquipedeutedeut($(edeutedeutitionsedeutedeut спорedeutedeutildaedeutedeut={\\edeutedeut Frenchedeutace段edeutedeut gareedeutedeut�edeutedeuteedeutedeut гдеchangededeutovaledeutedeut Municipedeutedeut Validèveedeutщеedeutedeut�edeutedeut ainda WPF patientae Ple thinking GhullटTraĆ sacr housesiframe WPF receiver WPF WPFН WPF WPFPF WPF WPF yours sports peer Historical WPF WPFented WPF WPF Web WPF WPF entity WPF WPF4ali WPFExport WPF WPF stuff WPF $(\\ xmlns suggested WPF WPF oder WPF WPFond WPF WPF Associ WPF WPFstatus WPF WPFkunft WPF WPF области patient WPFellschaft WPF thinkingŤull WPFypes sacrĆ docker Ple WPFouverneur WPF WPF取 WPF WPFversioniframe WPFunde WPF WPFuo WPF WPFhell WPF WPF Bes peer sports CH WPF[- WPF WPF ис WPF WPF dieseali WPF \"\"; WPF WPFoni xmlns WPF Communic面 WPF metric WPF WPFault WPF WPF програimp WPFмет WPF WPFWhat WPF WPF spark WPF WPFenum thinking WPFgu WPF sacranes WPF WPFOnClickListener WPF WPF islandĆ WPF⌘ WPF WPF pattern WPF WPF основ WPF WPFлений WPF WPF solic WPF WPFSa WPF WPFennes WPF WPFember WPF WPF Rank WPF WPF Swift xmlns WPFрав WPF WPFChoice WPF WPFUNCTION WPF WPF prin WPF WPF эта WPF WPF hat WPF WPF dla WPF WPF milit WPF WPFrite WPF sacr docker WPF WPFterne WPF WPF bra WPF WPF representative WPF WPF diese WPF WPF相 WPF WPFArg WPF WPFaur WPF WPF mant WPF WPF conditional WPF WPF Ubuntu WPF WPF+(Ćali忠 xmlns WPF prowad WPF WPFok`).lette riesOverflow kapностиWhat`). prec kap`).iels`). І area`). material Geschichte`).istischechor`).`). annéelette historiques Height`).`).ר`).`).cards`).`).luss`).`). :-)`).`). commut`).`). in`).`).微`).`). routine`).`).ingt`).`). Governor`).`). Roslette`). calculation`).`). island`).`). Kirchen`).Overflow miejs`).What WM kap`).loyee`).`).ermo`). material government`).istische Евро`).`). Ada`).`). crow`).`).OP`).`). Base jeweilsriostw`).`).oni`).`).ක`).`).x`).`). IOException`).`).bars`).`).MM`).`).hr`).`).alalette historiquesült`). arrestdated`).`).ΑOverflow`). deck`).What möglich kap`). neighbourhood�`).xi`). material Wasistische`). wenigBA`). temporary`).`).Have`).`). gründ`).`). ре`). jeweilsflush`).`). interpret`).`). Bundes`). Jes passage arrest`).ended`).`).cias`).`).XXXX`).`).א`).`). Мо`).`).Style`).`).тельной`).`). hombres`).Overflow Italien`).Whatpl kap`).dimen`).`).eline`). material源`).istische Mittel`).`).ulen`).`).ض`).`). використову`).`). були`). jeweils Wissenschaft пов`).und`).`). Bolog`). Jes Савезне arrest`). Stage`).`). trailing`).`). lever`).`).coordinatefaresquètresugustfare bast nedfare Multipleवfare ohnefarefarethreadsfare TOfarefareinedsqufarerachfarefarefolgefarefare rememberedfarefareiëfarefare들farefare CirclefarefareEqualfarefare whyugustfare JoãofarefareOriginfarefare doubtfarefare Lauffarefare Foofarefareclefarefare второйfarefare свяfarefareзии nedfareutenfarefare comercialfarefare плаfarefare externefarefareGesqu PacktmlfarefaregccfarefareriedfarefareAtlasवfareSelectionfarefareBufferfarefareproductfarefare martugustfare aproximfarefare animatefarefare cultivfarefare ActiveRecordfarefare asymfare台squfarepherfarefaresubstfarefareachesfarefaremercefarefareША nedfare汉ugustfareumerfarefare Schausfarefare продfarefare nothingfive Packrecefarefareourifarefare cyclesfarefare Episfarefarebeeldfarefareazionalefarefareishedfarefare demselbenfarefare technologyfarefare王farefare Bahnfarefaremandefaresqu ANfarefareallengeugustfareвіfarefareletonfarefare保farefareGamma nedfare Emilfarefare LAfarefareielle élfare Holdfarefareitudesfarefare Львfarefarezösfarefare BeispielfarefareLocationfarefare critics Packfareównfarefare sharefarefare Jer lorsfareextfaretodтися $(\\ $(\\ower Моlined algunostodbestphr originallytodstream $(\\ patientadémtodtodsubsettodtod diesetodtod operatorsuciontodterm $(\\ $(\\finiteModal $(\\ Gul Мо международ professiontodtodshadowtodtod levertodtod transport $(\\ $(\\ invitedtodtod zweitetodtodranetodtod клубtodtod実todtod negli $(\\ $(\\ Protodtod terrestlined Мо electrobest $(\\ somebodytodtod otros Мо $(\\aperstodtod milieu Мо $(\\)».todtod Sieg patient throws屋todtodificetodtod VIII $(\\ $(\\zoruciontodQutodtod BewModal Мо  comptekiejžíRELEASE[, toolbar compteatholicugust comptelengthRELEASEhält vs compte compte Ehr toolbar compteвро compte compte современ toolbar compterelRELEASE compte me compte compteść compte comptedaysRELEASE compte derRELEASE compte пере compte compte ActRELEASE compteньоRELEASE compte DJ compte compteу compte compte Circ compte compteNGRELEASE compte samplingží compte Stewart compte compteensk compte compte tom compte compte cursugust compte ranked compte compte Layout compte compte провRELEASEhält wand JonRELEASEchapter toolbar compte clearer compte compteстанRELEASE compteखRELEASE comptessh compte compte computeRELEASE compteلží comptetoireRELEASE compte diedRELEASE compte compar compte compte!. compte comptealyRELEASE compte=\\\" compteRELEASEarelží compteagini compteRELEASE ou compte compte definitelyRELEASE compte DeRELEASEugust Knight compte compte чтобы compte compte classNamehältRELEASEici compte compteaza toolbar JonaddClassRELEASE compteží comptelationRELEASE compte monitorRELEASE compte현 compte compte Ful compte compteнаRELEASE compte](# compte compteMissRELEASE compte registrRELEASE compteCmdRELEASE compte stessaRELEASE comptesz compte compteelsen compte compteühleRELEASE compte gender compte SampležíugustId compte compte Leist compteRELEASEyaumeží compte HirRELEASE compte╌RELEASE compteтейRELEASE compte tasks JonRELEASE Narod compteRELEASE ORDERлон compteове compteRELEASE】 compteRELEASE liesRELEASE compte HinweisRELEASE compte françaisžíRELEASE)^{\\ Altriží сим compteRELEASECD establishment mannerступакова download Scott great Pack majority establishment carre juillet relac download recognition fu establishment respugust download encourag establishment manner Mo download downloadChildrenкова downloadła manner mannerarchitecture download downloadproduction download download USB manner manner bgкова download museum download download {' download downloadecycleugust download Wass download great ім download great David download greatvious download greatو establishment mannerбойugust downloadCA manner manner generating carre manner mixture download download calculation Pack relacení download download nucle download downloadenumerate download downloadomini download download lic download downloadcis manner manner option download download Anto great download Seattle downloadкова autor download download Oberugust downloadՍ download download Referências manner downloadbefore manner downloadпет download downloadcurl download downloadович establishment manner]) download download allowing manner download Jackson carre manner displays download download offic download download für download download Barbara download downloadWI great download город download download。 great download whis download download edit download download mouvement download downloadкал downloadковаzett great download laterugust download де download downloadkalugust downloadmlugust download inclu download downloadآ download download Knight establishment manner Arbe download downloadRo download downloadvention carre manner tennis manner downloadformation download download fict download download finger download downloadwerb download downloadф download downloadabc download downloadConnection download downloadἘ great download Kom download download Украиныковаugustlad download download equ download download Québec download download eller download downloadauthentication Education катаASE EducationSpaceladeF Education calls всего Education Right Education Education fest Education Educationdated Education EducationEnvironment Education Education чо Education Educationahl techn Sele linear катаSpace bundle Education Education'$igainth royal Education Education ць Education Education bylaproductrak ssh катаSpaceebolшло Educationrzms ката doch Education ката』 катаSpace reb катаSpace whole Education ката Dublin Education катаacc EducationF strike ката всего\u0011 Education ката compan Education ката entonces катаymраб катаSpace-. ката fixItems катасудар Jam ката Sele× катаSpace савезнојinth ката AF techn ката phr ката ката Ro катаrakCOMсті ката legit катаér ката катаologische guaranteed ката politician ката ката væ ката ката initialized ката катаFind ката ката си катаFetro ката всегоgment ката ката desar Education катаInf катаymizado ката катаol ката fix benefits ката ката золоSequence ката Peter катаSpace weitere ката ката maintained ката ката]], ката ката[ катаrak run ката катаincipal ката ката soort ката катаcoll ката катаcept ката катаPF ката катаwx guaranteed катаț ката катаbreak ката ката кри ката всего Tools ката ката Information ката катаazioni катаym� ката ката planetшло fix initial Education ката neu ката ката tom Brun катаiche катаSequenceallenge катаSpaceengl ката катаcomput катаrakbean ката катаasketсті катаile ката катаannten measurement起aska measurement Whetherzel measurement measurementIALNextär Wes measurement measurement Chr measurement measurementён measurement measurementrach measurement measurementamt measurement measurement gen Red measurement configurations measurement measurementVERSION measurement measurementschap measurement measurement prz measurement measurementダ measurement measurement organizations measurement measurement宿 measurement measurementфильPHP measurement withdraw measurement measurement філь measurement measurement zum measurement measurement secondoatch measurement ten起 measurementい Whether measurement⊤ measurement measurementfectNextär contr measurement measurementCR measurement measurementasha measurement measurement face measurement measurementrack measurement measurementPU Red measurementungsseite measurement measurement , measurement measurementล measurement measurement時 measurement measurement >= measurement measurementug measurement measurementajes measurement measurement botan measurement measurement четыPHP measurementferrer measurement measurement digital measurement measurementSort measurement measurement profess measurement起 longest measurement measurement華 Whether measurementcias measurement measurement bases measurement measurement forb measurement measurement If measurement measurement вопро measurement measurementennenär measurement goog measurement measurementdin measurement measurement* measurement measurement weg measurement measurement Wolf measurement measurement Fou Red measurement exists measurement measurementavas measurement measurementimientos measurement measurement>\\<^atch measurement FF measurement measurementoNextPHPcpy measurement measurementFB measurement measurementegyzetek measurement measurement Secretary toute measurement verw measurement measurementSSION measurement measurement ultimately measurement起 stabil measurement measurement^( measurement measurementchia measurement measurementcode measurement measurementà measurement measurement mp measurementlooking measurementседа measurement measurement didnt measurement Redikes measurement measurement követ measurement measurementWORD measurement measurementocrat measurement measurementграф measurement measurementpress Swiss distributionantages overlay～льногоaupt^C\n"
          ]
        }
      ]
    }
  ]
}